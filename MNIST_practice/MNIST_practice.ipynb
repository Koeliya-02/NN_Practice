{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd3mnAI6lMMQ",
        "outputId": "34195c24-2bad-4271-d930-b200090a28ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/MNIST_practice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn93mfaPlN8f",
        "outputId": "94153202-2508-4469-e740-8c6f03498d0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MNIST_practice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv(\"MNIST/mnist_train.csv\")\n",
        "test = pd.read_csv(\"MNIST/mnist_test.csv\")\n"
      ],
      "metadata": {
        "id": "U2cUxGB1lYxW"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train))\n",
        "print(train.head(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpykCcgQmP5h",
        "outputId": "b1b0881d-7bea-4f02-cb31-3ba738e151a3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
            "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "\n",
            "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
            "0      0      0      0      0      0      0      0      0  \n",
            "\n",
            "[1 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#train = train.drop('label', axis=1)\n",
        "print(train.shape)\n",
        "print(train.head(1))\n",
        "print(train.iloc[0, 1:].values)\n",
        "plt.imshow(train.iloc[0, 1:].values.reshape(28, 28))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o0J3J5nyuVtr",
        "outputId": "addb4908-6e5a-4562-e31d-786444d7b864"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 785)\n",
            "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
            "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "\n",
            "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
            "0      0      0      0      0      0      0      0      0  \n",
            "\n",
            "[1 rows x 785 columns]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
            " 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
            " 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
            "   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
            "  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
            " 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
            " 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            " 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
            " 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
            "  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0b76591660>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "traind = train.drop('label', axis=1)\n",
        "print(traind.shape)\n",
        "print(traind.head(1))\n",
        "print(traind.iloc[0].values)\n",
        "plt.imshow(traind.iloc[5].values.reshape(28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uUlJaRoou5Ck",
        "outputId": "8adeb7db-1f88-45f0-fc26-b61e181da263"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "   1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  1x10  ...  28x19  28x20  \\\n",
            "0    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
            "\n",
            "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
            "0      0      0      0      0      0      0      0      0  \n",
            "\n",
            "[1 rows x 784 columns]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
            " 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
            " 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
            "   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
            "  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
            " 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
            " 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            " 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
            " 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
            "  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0b76464a00>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(traind)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "rKE_WHn7tO4p",
        "outputId": "962d2d9b-929a-4551-fdfc-04ccedeb9c21"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.frame.DataFrame</b><br/>def __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py</a>Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n",
              "\n",
              "Data structure also contains labeled axes (rows and columns).\n",
              "Arithmetic operations align on both row and column labels. Can be\n",
              "thought of as a dict-like container for Series objects. The primary\n",
              "pandas data structure.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n",
              "    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n",
              "    data is a dict, column order follows insertion-order. If a dict contains Series\n",
              "    which have an index defined, it is aligned by its index. This alignment also\n",
              "    occurs if data is a Series or a DataFrame itself. Alignment is done on\n",
              "    Series/DataFrame inputs.\n",
              "\n",
              "    If data is a list of dicts, column order follows insertion-order.\n",
              "\n",
              "index : Index or array-like\n",
              "    Index to use for resulting frame. Will default to RangeIndex if\n",
              "    no indexing information part of input data and no index provided.\n",
              "columns : Index or array-like\n",
              "    Column labels to use for resulting frame when data does not have them,\n",
              "    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n",
              "    will perform column selection instead.\n",
              "dtype : dtype, default None\n",
              "    Data type to force. Only a single dtype is allowed. If None, infer.\n",
              "copy : bool or None, default None\n",
              "    Copy data from inputs.\n",
              "    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n",
              "    or 2d ndarray input, the default of None behaves like ``copy=False``.\n",
              "    If data is a dict containing one or more Series (possibly of different dtypes),\n",
              "    ``copy=False`` will ensure that these inputs are not copied.\n",
              "\n",
              "    .. versionchanged:: 1.3.0\n",
              "\n",
              "See Also\n",
              "--------\n",
              "DataFrame.from_records : Constructor from tuples, also record arrays.\n",
              "DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n",
              "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
              "read_table : Read general delimited file into DataFrame.\n",
              "read_clipboard : Read text from clipboard into DataFrame.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Please reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing DataFrame from a dictionary.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [1, 2], &#x27;col2&#x27;: [3, 4]}\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d)\n",
              "&gt;&gt;&gt; df\n",
              "   col1  col2\n",
              "0     1     3\n",
              "1     2     4\n",
              "\n",
              "Notice that the inferred dtype is int64.\n",
              "\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int64\n",
              "col2    int64\n",
              "dtype: object\n",
              "\n",
              "To enforce a single dtype:\n",
              "\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int8\n",
              "col2    int8\n",
              "dtype: object\n",
              "\n",
              "Constructing DataFrame from a dictionary including Series:\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [0, 1, 2, 3], &#x27;col2&#x27;: pd.Series([2, 3], index=[2, 3])}\n",
              "&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n",
              "   col1  col2\n",
              "0     0   NaN\n",
              "1     1   NaN\n",
              "2     2   2.0\n",
              "3     3   3.0\n",
              "\n",
              "Constructing DataFrame from numpy ndarray:\n",
              "\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n",
              "...                    columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; df2\n",
              "   a  b  c\n",
              "0  1  2  3\n",
              "1  4  5  6\n",
              "2  7  8  9\n",
              "\n",
              "Constructing DataFrame from a numpy ndarray that has labeled columns:\n",
              "\n",
              "&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n",
              "...                 dtype=[(&quot;a&quot;, &quot;i4&quot;), (&quot;b&quot;, &quot;i4&quot;), (&quot;c&quot;, &quot;i4&quot;)])\n",
              "&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=[&#x27;c&#x27;, &#x27;a&#x27;])\n",
              "...\n",
              "&gt;&gt;&gt; df3\n",
              "   c  a\n",
              "0  3  1\n",
              "1  6  4\n",
              "2  9  7\n",
              "\n",
              "Constructing DataFrame from dataclass:\n",
              "\n",
              "&gt;&gt;&gt; from dataclasses import make_dataclass\n",
              "&gt;&gt;&gt; Point = make_dataclass(&quot;Point&quot;, [(&quot;x&quot;, int), (&quot;y&quot;, int)])\n",
              "&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n",
              "   x  y\n",
              "0  0  0\n",
              "1  0  3\n",
              "2  2  3\n",
              "\n",
              "Constructing DataFrame from Series/DataFrame:\n",
              "\n",
              "&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df\n",
              "   0\n",
              "a  1\n",
              "c  3\n",
              "\n",
              "&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], columns=[&quot;x&quot;])\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df2\n",
              "   x\n",
              "a  1\n",
              "c  3</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 490);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.iloc[0:,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3O1W2VIvgtS",
        "outputId": "a82b6a1f-9ba8-49a5-c189-f381c71609d5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        5\n",
            "1        0\n",
            "2        4\n",
            "3        1\n",
            "4        9\n",
            "        ..\n",
            "59995    8\n",
            "59996    3\n",
            "59997    5\n",
            "59998    6\n",
            "59999    8\n",
            "Name: label, Length: 60000, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = train.iloc[0:,0]\n",
        "test_labels = test.iloc[0:,0]\n",
        "train_data = train.iloc[0:,1:]\n",
        "test_data = test.iloc[0:,1:]"
      ],
      "metadata": {
        "id": "8-i7nfKR_Dgu"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "train_data = torch.tensor(train_data.to_numpy()).float()\n",
        "train_labels = torch.tensor(train_labels.to_numpy()).long()  # labels are usually long for classification tasks\n",
        "test_data = torch.tensor(test_data.to_numpy()).float()\n",
        "test_labels = torch.tensor(test_labels.to_numpy()).long()"
      ],
      "metadata": {
        "id": "c5cBhyGG_VRn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Xx_OJ84n_5KH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 254),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(254, 150),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(150, 10),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    output = F.log_softmax(logits, dim=1)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "RAKqVwIoBKPr"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umcGcOtBCOqJ",
        "outputId": "91855e67-5d91-4a77-bb1f-c3b4a4995560"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rdgzuAXYFz2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlN-CAq4CXyX",
        "outputId": "4738b0e6-6c54-4b15-fd04-83261b9a5f89"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=254, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=254, out_features=150, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=150, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_iao2SYF1Xc",
        "outputId": "979b63f0-2193-4f1c-daf6-e37c22fc1dc7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data[0,:].view(1,28,28)\n",
        "pred_probab = model(X)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")\n",
        "print(f\"Actual class: {train_labels[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXrqjlwCCbZe",
        "outputId": "327cbd4e-a17a-4d45-831f-93a3b8ce19c7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([9])\n",
            "Actual class: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "G0v_LLrSDgo0"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "#dataloader\n",
        "train_dataset = TensorDataset(train_data, train_labels)\n",
        "test_dataset = TensorDataset(test_data, test_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "EWjyy8wSIs8q"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# Define the testing loop\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "iC8nc1q6JLQI"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUFPuTR2JGtm",
        "outputId": "5460b10a-08d0-43a8-8ef3-e0934a69400a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.121001  [    0/60000]\n",
            "loss: 0.023506  [ 6400/60000]\n",
            "loss: 0.036275  [12800/60000]\n",
            "loss: 0.017593  [19200/60000]\n",
            "loss: 0.035224  [25600/60000]\n",
            "loss: 0.012120  [32000/60000]\n",
            "loss: 0.051604  [38400/60000]\n",
            "loss: 0.010173  [44800/60000]\n",
            "loss: 0.016638  [51200/60000]\n",
            "loss: 0.050330  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.094783 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.026007  [    0/60000]\n",
            "loss: 0.013788  [ 6400/60000]\n",
            "loss: 0.019524  [12800/60000]\n",
            "loss: 0.020223  [19200/60000]\n",
            "loss: 0.015501  [25600/60000]\n",
            "loss: 0.065042  [32000/60000]\n",
            "loss: 0.013645  [38400/60000]\n",
            "loss: 0.028476  [44800/60000]\n",
            "loss: 0.033190  [51200/60000]\n",
            "loss: 0.033175  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 0.090109 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.027742  [    0/60000]\n",
            "loss: 0.009672  [ 6400/60000]\n",
            "loss: 0.062183  [12800/60000]\n",
            "loss: 0.016380  [19200/60000]\n",
            "loss: 0.013529  [25600/60000]\n",
            "loss: 0.013463  [32000/60000]\n",
            "loss: 0.010242  [38400/60000]\n",
            "loss: 0.013294  [44800/60000]\n",
            "loss: 0.013716  [51200/60000]\n",
            "loss: 0.003455  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.088349 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.004347  [    0/60000]\n",
            "loss: 0.021144  [ 6400/60000]\n",
            "loss: 0.014247  [12800/60000]\n",
            "loss: 0.012351  [19200/60000]\n",
            "loss: 0.043597  [25600/60000]\n",
            "loss: 0.036161  [32000/60000]\n",
            "loss: 0.062410  [38400/60000]\n",
            "loss: 0.019792  [44800/60000]\n",
            "loss: 0.049449  [51200/60000]\n",
            "loss: 0.067008  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 0.100720 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.023894  [    0/60000]\n",
            "loss: 0.014666  [ 6400/60000]\n",
            "loss: 0.034620  [12800/60000]\n",
            "loss: 0.012275  [19200/60000]\n",
            "loss: 0.011127  [25600/60000]\n",
            "loss: 0.005411  [32000/60000]\n",
            "loss: 0.018743  [38400/60000]\n",
            "loss: 0.023180  [44800/60000]\n",
            "loss: 0.013645  [51200/60000]\n",
            "loss: 0.011859  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.087603 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.059585  [    0/60000]\n",
            "loss: 0.009314  [ 6400/60000]\n",
            "loss: 0.006140  [12800/60000]\n",
            "loss: 0.021979  [19200/60000]\n",
            "loss: 0.007915  [25600/60000]\n",
            "loss: 0.013144  [32000/60000]\n",
            "loss: 0.009237  [38400/60000]\n",
            "loss: 0.070294  [44800/60000]\n",
            "loss: 0.046128  [51200/60000]\n",
            "loss: 0.010847  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.087719 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.019525  [    0/60000]\n",
            "loss: 0.004885  [ 6400/60000]\n",
            "loss: 0.020621  [12800/60000]\n",
            "loss: 0.026188  [19200/60000]\n",
            "loss: 0.038542  [25600/60000]\n",
            "loss: 0.008989  [32000/60000]\n",
            "loss: 0.013195  [38400/60000]\n",
            "loss: 0.003228  [44800/60000]\n",
            "loss: 0.064354  [51200/60000]\n",
            "loss: 0.008188  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.090556 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.009601  [    0/60000]\n",
            "loss: 0.008493  [ 6400/60000]\n",
            "loss: 0.010982  [12800/60000]\n",
            "loss: 0.009206  [19200/60000]\n",
            "loss: 0.019780  [25600/60000]\n",
            "loss: 0.011999  [32000/60000]\n",
            "loss: 0.092247  [38400/60000]\n",
            "loss: 0.004771  [44800/60000]\n",
            "loss: 0.009720  [51200/60000]\n",
            "loss: 0.010012  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.085934 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.006945  [    0/60000]\n",
            "loss: 0.011420  [ 6400/60000]\n",
            "loss: 0.016906  [12800/60000]\n",
            "loss: 0.022750  [19200/60000]\n",
            "loss: 0.006238  [25600/60000]\n",
            "loss: 0.009844  [32000/60000]\n",
            "loss: 0.013270  [38400/60000]\n",
            "loss: 0.004637  [44800/60000]\n",
            "loss: 0.021336  [51200/60000]\n",
            "loss: 0.009972  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.090745 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.020691  [    0/60000]\n",
            "loss: 0.003821  [ 6400/60000]\n",
            "loss: 0.010993  [12800/60000]\n",
            "loss: 0.011701  [19200/60000]\n",
            "loss: 0.022993  [25600/60000]\n",
            "loss: 0.013208  [32000/60000]\n",
            "loss: 0.014876  [38400/60000]\n",
            "loss: 0.006484  [44800/60000]\n",
            "loss: 0.022098  [51200/60000]\n",
            "loss: 0.005883  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.087029 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data[0,:].view(1,28,28)\n",
        "pred_probab = model(X)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred.item()}\")\n",
        "print(f\"Actual class: {train_labels[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2z6WKROJnOv",
        "outputId": "1b7bff06-4fcd-4404-a441-f4930cf8ad22"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 5\n",
            "Actual class: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0ZwmyXFKIuE",
        "outputId": "2b04d25e-1086-48dd-f07d-b8cf323ae5bf"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.003899  [    0/60000]\n",
            "loss: 0.006878  [ 6400/60000]\n",
            "loss: 0.019217  [12800/60000]\n",
            "loss: 0.003643  [19200/60000]\n",
            "loss: 0.006181  [25600/60000]\n",
            "loss: 0.010996  [32000/60000]\n",
            "loss: 0.002389  [38400/60000]\n",
            "loss: 0.008036  [44800/60000]\n",
            "loss: 0.003596  [51200/60000]\n",
            "loss: 0.002374  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.085901 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.003161  [    0/60000]\n",
            "loss: 0.003578  [ 6400/60000]\n",
            "loss: 0.002512  [12800/60000]\n",
            "loss: 0.010120  [19200/60000]\n",
            "loss: 0.002613  [25600/60000]\n",
            "loss: 0.007292  [32000/60000]\n",
            "loss: 0.006431  [38400/60000]\n",
            "loss: 0.003035  [44800/60000]\n",
            "loss: 0.011968  [51200/60000]\n",
            "loss: 0.004176  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.085696 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.003206  [    0/60000]\n",
            "loss: 0.007400  [ 6400/60000]\n",
            "loss: 0.005422  [12800/60000]\n",
            "loss: 0.001556  [19200/60000]\n",
            "loss: 0.005173  [25600/60000]\n",
            "loss: 0.002653  [32000/60000]\n",
            "loss: 0.000776  [38400/60000]\n",
            "loss: 0.004371  [44800/60000]\n",
            "loss: 0.002952  [51200/60000]\n",
            "loss: 0.009385  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.085538 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.003026  [    0/60000]\n",
            "loss: 0.004548  [ 6400/60000]\n",
            "loss: 0.005942  [12800/60000]\n",
            "loss: 0.005217  [19200/60000]\n",
            "loss: 0.003912  [25600/60000]\n",
            "loss: 0.004131  [32000/60000]\n",
            "loss: 0.007802  [38400/60000]\n",
            "loss: 0.010766  [44800/60000]\n",
            "loss: 0.011664  [51200/60000]\n",
            "loss: 0.004124  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.085603 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.011956  [    0/60000]\n",
            "loss: 0.007632  [ 6400/60000]\n",
            "loss: 0.001154  [12800/60000]\n",
            "loss: 0.004145  [19200/60000]\n",
            "loss: 0.002661  [25600/60000]\n",
            "loss: 0.005960  [32000/60000]\n",
            "loss: 0.002915  [38400/60000]\n",
            "loss: 0.008691  [44800/60000]\n",
            "loss: 0.006350  [51200/60000]\n",
            "loss: 0.005139  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.085577 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.013122  [    0/60000]\n",
            "loss: 0.004472  [ 6400/60000]\n",
            "loss: 0.008382  [12800/60000]\n",
            "loss: 0.004988  [19200/60000]\n",
            "loss: 0.007030  [25600/60000]\n",
            "loss: 0.006839  [32000/60000]\n",
            "loss: 0.003554  [38400/60000]\n",
            "loss: 0.001545  [44800/60000]\n",
            "loss: 0.011451  [51200/60000]\n",
            "loss: 0.005591  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.085632 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.002798  [    0/60000]\n",
            "loss: 0.008074  [ 6400/60000]\n",
            "loss: 0.001064  [12800/60000]\n",
            "loss: 0.004376  [19200/60000]\n",
            "loss: 0.001743  [25600/60000]\n",
            "loss: 0.005482  [32000/60000]\n",
            "loss: 0.008271  [38400/60000]\n",
            "loss: 0.003731  [44800/60000]\n",
            "loss: 0.005335  [51200/60000]\n",
            "loss: 0.001201  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.085723 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.002213  [    0/60000]\n",
            "loss: 0.002549  [ 6400/60000]\n",
            "loss: 0.002218  [12800/60000]\n",
            "loss: 0.006137  [19200/60000]\n",
            "loss: 0.003402  [25600/60000]\n",
            "loss: 0.000616  [32000/60000]\n",
            "loss: 0.007177  [38400/60000]\n",
            "loss: 0.012017  [44800/60000]\n",
            "loss: 0.000787  [51200/60000]\n",
            "loss: 0.003562  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.085595 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.004567  [    0/60000]\n",
            "loss: 0.003888  [ 6400/60000]\n",
            "loss: 0.001089  [12800/60000]\n",
            "loss: 0.006942  [19200/60000]\n",
            "loss: 0.012986  [25600/60000]\n",
            "loss: 0.002957  [32000/60000]\n",
            "loss: 0.004484  [38400/60000]\n",
            "loss: 0.004982  [44800/60000]\n",
            "loss: 0.003741  [51200/60000]\n",
            "loss: 0.003323  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.085855 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.005447  [    0/60000]\n",
            "loss: 0.003652  [ 6400/60000]\n",
            "loss: 0.003051  [12800/60000]\n",
            "loss: 0.004299  [19200/60000]\n",
            "loss: 0.003285  [25600/60000]\n",
            "loss: 0.010778  [32000/60000]\n",
            "loss: 0.011841  [38400/60000]\n",
            "loss: 0.004721  [44800/60000]\n",
            "loss: 0.010290  [51200/60000]\n",
            "loss: 0.013194  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.085722 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data[0,:].view(1,28,28)\n",
        "pred_probab = model(X)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred.item()}\")\n",
        "print(f\"Actual class: {train_labels[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJEwxU19LLPy",
        "outputId": "1eb1086a-f046-4bac-9506-5ca45b0d979d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 5\n",
            "Actual class: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuMAR-WUK4dx",
        "outputId": "8169e9dd-cdfb-46e9-bfb4-04dc985c1fd4"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.006009  [    0/60000]\n",
            "loss: 0.012039  [ 6400/60000]\n",
            "loss: 0.001371  [12800/60000]\n",
            "loss: 0.011131  [19200/60000]\n",
            "loss: 0.001744  [25600/60000]\n",
            "loss: 0.005565  [32000/60000]\n",
            "loss: 0.005533  [38400/60000]\n",
            "loss: 0.003139  [44800/60000]\n",
            "loss: 0.012239  [51200/60000]\n",
            "loss: 0.005299  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.087471 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.018672  [    0/60000]\n",
            "loss: 0.003564  [ 6400/60000]\n",
            "loss: 0.002169  [12800/60000]\n",
            "loss: 0.003051  [19200/60000]\n",
            "loss: 0.003182  [25600/60000]\n",
            "loss: 0.010111  [32000/60000]\n",
            "loss: 0.005246  [38400/60000]\n",
            "loss: 0.009631  [44800/60000]\n",
            "loss: 0.006203  [51200/60000]\n",
            "loss: 0.003117  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.087275 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.002162  [    0/60000]\n",
            "loss: 0.005631  [ 6400/60000]\n",
            "loss: 0.005073  [12800/60000]\n",
            "loss: 0.001991  [19200/60000]\n",
            "loss: 0.004420  [25600/60000]\n",
            "loss: 0.011679  [32000/60000]\n",
            "loss: 0.003431  [38400/60000]\n",
            "loss: 0.004180  [44800/60000]\n",
            "loss: 0.006458  [51200/60000]\n",
            "loss: 0.007790  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.088034 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.001338  [    0/60000]\n",
            "loss: 0.011278  [ 6400/60000]\n",
            "loss: 0.002907  [12800/60000]\n",
            "loss: 0.003429  [19200/60000]\n",
            "loss: 0.004700  [25600/60000]\n",
            "loss: 0.003558  [32000/60000]\n",
            "loss: 0.003206  [38400/60000]\n",
            "loss: 0.002479  [44800/60000]\n",
            "loss: 0.010291  [51200/60000]\n",
            "loss: 0.004612  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.088043 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.003401  [    0/60000]\n",
            "loss: 0.002363  [ 6400/60000]\n",
            "loss: 0.002796  [12800/60000]\n",
            "loss: 0.003364  [19200/60000]\n",
            "loss: 0.003618  [25600/60000]\n",
            "loss: 0.004693  [32000/60000]\n",
            "loss: 0.004208  [38400/60000]\n",
            "loss: 0.003855  [44800/60000]\n",
            "loss: 0.001198  [51200/60000]\n",
            "loss: 0.007703  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.087236 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.006025  [    0/60000]\n",
            "loss: 0.004108  [ 6400/60000]\n",
            "loss: 0.004032  [12800/60000]\n",
            "loss: 0.002021  [19200/60000]\n",
            "loss: 0.004444  [25600/60000]\n",
            "loss: 0.003336  [32000/60000]\n",
            "loss: 0.003241  [38400/60000]\n",
            "loss: 0.006586  [44800/60000]\n",
            "loss: 0.001944  [51200/60000]\n",
            "loss: 0.005611  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.087023 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.007588  [    0/60000]\n",
            "loss: 0.004531  [ 6400/60000]\n",
            "loss: 0.005064  [12800/60000]\n",
            "loss: 0.002261  [19200/60000]\n",
            "loss: 0.003246  [25600/60000]\n",
            "loss: 0.003037  [32000/60000]\n",
            "loss: 0.008422  [38400/60000]\n",
            "loss: 0.001944  [44800/60000]\n",
            "loss: 0.002199  [51200/60000]\n",
            "loss: 0.002054  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.088230 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.003816  [    0/60000]\n",
            "loss: 0.004682  [ 6400/60000]\n",
            "loss: 0.005817  [12800/60000]\n",
            "loss: 0.002670  [19200/60000]\n",
            "loss: 0.009374  [25600/60000]\n",
            "loss: 0.002857  [32000/60000]\n",
            "loss: 0.003778  [38400/60000]\n",
            "loss: 0.001184  [44800/60000]\n",
            "loss: 0.002875  [51200/60000]\n",
            "loss: 0.002441  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.087989 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.005228  [    0/60000]\n",
            "loss: 0.006109  [ 6400/60000]\n",
            "loss: 0.001930  [12800/60000]\n",
            "loss: 0.002948  [19200/60000]\n",
            "loss: 0.006476  [25600/60000]\n",
            "loss: 0.001024  [32000/60000]\n",
            "loss: 0.005099  [38400/60000]\n",
            "loss: 0.003410  [44800/60000]\n",
            "loss: 0.004003  [51200/60000]\n",
            "loss: 0.001643  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.087706 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.003865  [    0/60000]\n",
            "loss: 0.002966  [ 6400/60000]\n",
            "loss: 0.003625  [12800/60000]\n",
            "loss: 0.005953  [19200/60000]\n",
            "loss: 0.001072  [25600/60000]\n",
            "loss: 0.006060  [32000/60000]\n",
            "loss: 0.003994  [38400/60000]\n",
            "loss: 0.002279  [44800/60000]\n",
            "loss: 0.001500  [51200/60000]\n",
            "loss: 0.001702  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.088661 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data[0,:].view(1,28,28)\n",
        "pred_probab = model(X)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred.item()}\")\n",
        "print(f\"Actual class: {train_labels[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpqeZ5MELMCl",
        "outputId": "0e0ec3c1-6637-4f29-f0d3-5acd947a8901"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 5\n",
            "Actual class: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 15\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z27U0DjaK6bp",
        "outputId": "90327478-f033-4a0c-a15d-1802c020f6be"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.005324  [    0/60000]\n",
            "loss: 0.003678  [ 6400/60000]\n",
            "loss: 0.004281  [12800/60000]\n",
            "loss: 0.000748  [19200/60000]\n",
            "loss: 0.000895  [25600/60000]\n",
            "loss: 0.001239  [32000/60000]\n",
            "loss: 0.002513  [38400/60000]\n",
            "loss: 0.002195  [44800/60000]\n",
            "loss: 0.003309  [51200/60000]\n",
            "loss: 0.003901  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.088267 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.004278  [    0/60000]\n",
            "loss: 0.000980  [ 6400/60000]\n",
            "loss: 0.001098  [12800/60000]\n",
            "loss: 0.001934  [19200/60000]\n",
            "loss: 0.004600  [25600/60000]\n",
            "loss: 0.003465  [32000/60000]\n",
            "loss: 0.000521  [38400/60000]\n",
            "loss: 0.003855  [44800/60000]\n",
            "loss: 0.004440  [51200/60000]\n",
            "loss: 0.005258  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.088384 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.002583  [    0/60000]\n",
            "loss: 0.002379  [ 6400/60000]\n",
            "loss: 0.002172  [12800/60000]\n",
            "loss: 0.018514  [19200/60000]\n",
            "loss: 0.002183  [25600/60000]\n",
            "loss: 0.004559  [32000/60000]\n",
            "loss: 0.003382  [38400/60000]\n",
            "loss: 0.002044  [44800/60000]\n",
            "loss: 0.004534  [51200/60000]\n",
            "loss: 0.000732  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.090299 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.011052  [    0/60000]\n",
            "loss: 0.001471  [ 6400/60000]\n",
            "loss: 0.000320  [12800/60000]\n",
            "loss: 0.001971  [19200/60000]\n",
            "loss: 0.003346  [25600/60000]\n",
            "loss: 0.003159  [32000/60000]\n",
            "loss: 0.002612  [38400/60000]\n",
            "loss: 0.001136  [44800/60000]\n",
            "loss: 0.002463  [51200/60000]\n",
            "loss: 0.002887  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.089097 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.001744  [    0/60000]\n",
            "loss: 0.002638  [ 6400/60000]\n",
            "loss: 0.001547  [12800/60000]\n",
            "loss: 0.002019  [19200/60000]\n",
            "loss: 0.002141  [25600/60000]\n",
            "loss: 0.001333  [32000/60000]\n",
            "loss: 0.001016  [38400/60000]\n",
            "loss: 0.001789  [44800/60000]\n",
            "loss: 0.002292  [51200/60000]\n",
            "loss: 0.002858  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.089020 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.002351  [    0/60000]\n",
            "loss: 0.003380  [ 6400/60000]\n",
            "loss: 0.002298  [12800/60000]\n",
            "loss: 0.001343  [19200/60000]\n",
            "loss: 0.002059  [25600/60000]\n",
            "loss: 0.002775  [32000/60000]\n",
            "loss: 0.002483  [38400/60000]\n",
            "loss: 0.001609  [44800/60000]\n",
            "loss: 0.001683  [51200/60000]\n",
            "loss: 0.001412  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.089115 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.001482  [    0/60000]\n",
            "loss: 0.000967  [ 6400/60000]\n",
            "loss: 0.000791  [12800/60000]\n",
            "loss: 0.001892  [19200/60000]\n",
            "loss: 0.003558  [25600/60000]\n",
            "loss: 0.001745  [32000/60000]\n",
            "loss: 0.005519  [38400/60000]\n",
            "loss: 0.000680  [44800/60000]\n",
            "loss: 0.001149  [51200/60000]\n",
            "loss: 0.001775  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.090736 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.002545  [    0/60000]\n",
            "loss: 0.004103  [ 6400/60000]\n",
            "loss: 0.001262  [12800/60000]\n",
            "loss: 0.002383  [19200/60000]\n",
            "loss: 0.001872  [25600/60000]\n",
            "loss: 0.002819  [32000/60000]\n",
            "loss: 0.001689  [38400/60000]\n",
            "loss: 0.002432  [44800/60000]\n",
            "loss: 0.001972  [51200/60000]\n",
            "loss: 0.002086  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.089571 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.000962  [    0/60000]\n",
            "loss: 0.002828  [ 6400/60000]\n",
            "loss: 0.001571  [12800/60000]\n",
            "loss: 0.001437  [19200/60000]\n",
            "loss: 0.001265  [25600/60000]\n",
            "loss: 0.001588  [32000/60000]\n",
            "loss: 0.003343  [38400/60000]\n",
            "loss: 0.003497  [44800/60000]\n",
            "loss: 0.002548  [51200/60000]\n",
            "loss: 0.001414  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.089779 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.001637  [    0/60000]\n",
            "loss: 0.001437  [ 6400/60000]\n",
            "loss: 0.002748  [12800/60000]\n",
            "loss: 0.003908  [19200/60000]\n",
            "loss: 0.002203  [25600/60000]\n",
            "loss: 0.002172  [32000/60000]\n",
            "loss: 0.002783  [38400/60000]\n",
            "loss: 0.002885  [44800/60000]\n",
            "loss: 0.002299  [51200/60000]\n",
            "loss: 0.002127  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.090074 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.001206  [    0/60000]\n",
            "loss: 0.001367  [ 6400/60000]\n",
            "loss: 0.000961  [12800/60000]\n",
            "loss: 0.000852  [19200/60000]\n",
            "loss: 0.000809  [25600/60000]\n",
            "loss: 0.002468  [32000/60000]\n",
            "loss: 0.002276  [38400/60000]\n",
            "loss: 0.001643  [44800/60000]\n",
            "loss: 0.002198  [51200/60000]\n",
            "loss: 0.003225  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.090197 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.002357  [    0/60000]\n",
            "loss: 0.001829  [ 6400/60000]\n",
            "loss: 0.005584  [12800/60000]\n",
            "loss: 0.003495  [19200/60000]\n",
            "loss: 0.001413  [25600/60000]\n",
            "loss: 0.001471  [32000/60000]\n",
            "loss: 0.001106  [38400/60000]\n",
            "loss: 0.001216  [44800/60000]\n",
            "loss: 0.002060  [51200/60000]\n",
            "loss: 0.000174  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.090321 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.000566  [    0/60000]\n",
            "loss: 0.001666  [ 6400/60000]\n",
            "loss: 0.001183  [12800/60000]\n",
            "loss: 0.000760  [19200/60000]\n",
            "loss: 0.001328  [25600/60000]\n",
            "loss: 0.001621  [32000/60000]\n",
            "loss: 0.002544  [38400/60000]\n",
            "loss: 0.000829  [44800/60000]\n",
            "loss: 0.003220  [51200/60000]\n",
            "loss: 0.001721  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.090643 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.000343  [    0/60000]\n",
            "loss: 0.000910  [ 6400/60000]\n",
            "loss: 0.000588  [12800/60000]\n",
            "loss: 0.001868  [19200/60000]\n",
            "loss: 0.005519  [25600/60000]\n",
            "loss: 0.001190  [32000/60000]\n",
            "loss: 0.003354  [38400/60000]\n",
            "loss: 0.001635  [44800/60000]\n",
            "loss: 0.001150  [51200/60000]\n",
            "loss: 0.001371  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.090465 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.001121  [    0/60000]\n",
            "loss: 0.000878  [ 6400/60000]\n",
            "loss: 0.001308  [12800/60000]\n",
            "loss: 0.001323  [19200/60000]\n",
            "loss: 0.001678  [25600/60000]\n",
            "loss: 0.003427  [32000/60000]\n",
            "loss: 0.000617  [38400/60000]\n",
            "loss: 0.001364  [44800/60000]\n",
            "loss: 0.004151  [51200/60000]\n",
            "loss: 0.001525  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.091284 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data[0,:].view(1,28,28)\n",
        "pred_probab = model(X)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred.item()}\")\n",
        "print(f\"Actual class: {train_labels[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZXqTFBBLMxe",
        "outputId": "92f1c53d-c859-4017-8463-1c3b7af96700"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 5\n",
            "Actual class: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEB_6A6hK-JG",
        "outputId": "85d98127-7420-455c-9c39-d2e855658cb4"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.001740  [    0/60000]\n",
            "loss: 0.000777  [ 6400/60000]\n",
            "loss: 0.000382  [12800/60000]\n",
            "loss: 0.000842  [19200/60000]\n",
            "loss: 0.004202  [25600/60000]\n",
            "loss: 0.000581  [32000/60000]\n",
            "loss: 0.000725  [38400/60000]\n",
            "loss: 0.001006  [44800/60000]\n",
            "loss: 0.001779  [51200/60000]\n",
            "loss: 0.001658  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.091138 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.002072  [    0/60000]\n",
            "loss: 0.000705  [ 6400/60000]\n",
            "loss: 0.002622  [12800/60000]\n",
            "loss: 0.002219  [19200/60000]\n",
            "loss: 0.000386  [25600/60000]\n",
            "loss: 0.001314  [32000/60000]\n",
            "loss: 0.000276  [38400/60000]\n",
            "loss: 0.000336  [44800/60000]\n",
            "loss: 0.000798  [51200/60000]\n",
            "loss: 0.001355  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.091704 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.002183  [    0/60000]\n",
            "loss: 0.001305  [ 6400/60000]\n",
            "loss: 0.000879  [12800/60000]\n",
            "loss: 0.000860  [19200/60000]\n",
            "loss: 0.000513  [25600/60000]\n",
            "loss: 0.001441  [32000/60000]\n",
            "loss: 0.000885  [38400/60000]\n",
            "loss: 0.001062  [44800/60000]\n",
            "loss: 0.001434  [51200/60000]\n",
            "loss: 0.001432  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.091460 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.001342  [    0/60000]\n",
            "loss: 0.000514  [ 6400/60000]\n",
            "loss: 0.001639  [12800/60000]\n",
            "loss: 0.000925  [19200/60000]\n",
            "loss: 0.002043  [25600/60000]\n",
            "loss: 0.001511  [32000/60000]\n",
            "loss: 0.001758  [38400/60000]\n",
            "loss: 0.001604  [44800/60000]\n",
            "loss: 0.001342  [51200/60000]\n",
            "loss: 0.001459  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.091744 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.002124  [    0/60000]\n",
            "loss: 0.000747  [ 6400/60000]\n",
            "loss: 0.002310  [12800/60000]\n",
            "loss: 0.000879  [19200/60000]\n",
            "loss: 0.002869  [25600/60000]\n",
            "loss: 0.000762  [32000/60000]\n",
            "loss: 0.002132  [38400/60000]\n",
            "loss: 0.001753  [44800/60000]\n",
            "loss: 0.001196  [51200/60000]\n",
            "loss: 0.002715  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.092036 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = test_data[0,:].view(1,28,28)\n",
        "pred_probab = model(X)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred.item()}\")\n",
        "print(f\"Actual class: {test_labels[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2Bev9GHK_7V",
        "outputId": "0faf6ea2-ad55-487b-ba0d-e9a0a9373554"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 7\n",
            "Actual class: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ],
      "metadata": {
        "id": "QBxzRBJeLREl"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = Net()\n",
        "loaded_model.load_state_dict(torch.load('model_weights.pth'))\n",
        "loaded_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq_kHii2NyDE",
        "outputId": "a815f0dd-6dec-4867-a916-2469e38214da"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_relu_stack): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=254, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=254, out_features=150, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=150, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test Accuracy of loaded model:\")\n",
        "test_loop(test_dataloader, model, loss_fn)\n",
        "count=0\n",
        "for i in range(10):\n",
        "  X = test_data[i,:].view(1,28,28)\n",
        "  pred_probab = loaded_model(X)\n",
        "  y_pred = pred_probab.argmax(1)\n",
        "  print(f\"Predicted class: {y_pred.item()}\")\n",
        "  print(f\"Actual class: {test_labels[i]}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBKyn1a0N6AP",
        "outputId": "38edf56b-42e6-4559-abef-509577d9bf05"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of loaded model:\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.092036 \n",
            "\n",
            "Predicted class: 7\n",
            "Actual class: 7\n",
            "\n",
            "Predicted class: 2\n",
            "Actual class: 2\n",
            "\n",
            "Predicted class: 1\n",
            "Actual class: 1\n",
            "\n",
            "Predicted class: 0\n",
            "Actual class: 0\n",
            "\n",
            "Predicted class: 4\n",
            "Actual class: 4\n",
            "\n",
            "Predicted class: 1\n",
            "Actual class: 1\n",
            "\n",
            "Predicted class: 4\n",
            "Actual class: 4\n",
            "\n",
            "Predicted class: 9\n",
            "Actual class: 9\n",
            "\n",
            "Predicted class: 6\n",
            "Actual class: 5\n",
            "\n",
            "Predicted class: 9\n",
            "Actual class: 9\n",
            "\n"
          ]
        }
      ]
    }
  ]
}